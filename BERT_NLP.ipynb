{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_NLP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AYdKpai3A54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0caa7c8-21c4-4bb5-c5f3-8500a9cc2d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 495.0 MB 30 kB/s \n",
            "\u001b[K     |████████████████████████████████| 463 kB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 48.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 54.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 48.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.8 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text==2.7.3\n",
        "!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "8VHrgFy6A9fS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "isC5kwk3CTbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to get the data in roughly equal proportions, so we drop some examples so that classes are at most twice as large as each other."
      ],
      "metadata": {
        "id": "r93OgLiwCTeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_count_diff = 10\n",
        "import csv\n",
        "\n",
        "df = pd.read_csv('train-data.csv')\n",
        "max_count = df['Topic'].value_counts().min() * max_count_diff\n",
        "cats = np.array(pd.Categorical(df['Topic']).categories)\n",
        "\n",
        "new_df = pd.DataFrame(columns=df.columns)\n",
        "for cat in cats:\n",
        "  values = df[df['Topic']==cat]\n",
        "  new_df = new_df.append(values.sample(min(max_count, len(values))))\n",
        "\n",
        "# df[['Topic']] = df[['Topic']].apply(lambda col: pd.Categorical(col).codes)"
      ],
      "metadata": {
        "id": "AiwlTMHSCeK3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "\n",
        "val_split = 0.2\n",
        "train_split = 1 - val_split\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=val_split)\n",
        "\n",
        "def make_pred_vector(topics):\n",
        "  pred_categories = [cat for cat in cats if cat != 'none']\n",
        "  result = np.zeros((len(topics), len(pred_categories)))\n",
        "  for i, value in enumerate(topics):\n",
        "    if value != 'none':\n",
        "      result[i][pred_categories.index(value)] = 1.0\n",
        "\n",
        "  return result\n",
        "\n",
        "def make_dataset(df):\n",
        "  x = df['Tweet']\n",
        "  y = df['Topic']\n",
        "\n",
        "  y = make_pred_vector(y)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  ds = ds.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
        "  return ds\n",
        "\n",
        "train_ds = make_dataset(train_df)\n",
        "val_ds = make_dataset(val_df)\n"
      ],
      "metadata": {
        "id": "fTn_2bgxA_BO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.models.Model):\n",
        "  def __init__(self, categories, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "    bert_preprocess_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "    bert_model_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "\n",
        "    self.bert_preprocess = hub.KerasLayer(bert_preprocess_url)\n",
        "    self.bert_model = hub.KerasLayer(bert_model_url)\n",
        "\n",
        "    self.categories = [cat for cat in categories if cat != 'none']\n",
        "    self.num_classes = len(self.categories)\n",
        "\n",
        "    self.dense_layers = [\n",
        "      tf.keras.layers.Dense(1024),\n",
        "      tf.keras.activations.relu,\n",
        "      tf.keras.layers.Dense(self.num_classes),\n",
        "      tf.keras.activations.softmax,\n",
        "    ]\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.bert_preprocess(x)\n",
        "    x = self.bert_model(x)['pooled_output']\n",
        "    for layer in self.dense_layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  def decode_predictions(self, x):\n",
        "    return {self.categories[i]:x[i] for i in range(len(self.num_classes))}\n"
      ],
      "metadata": {
        "id": "fN2Lt25sKoGZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "# strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "validation_steps = tf.data.experimental.cardinality(val_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# with strategy.scope():\n",
        "model = Model(cats)\n",
        "model.compile(optimizer=optimizer, loss=loss, steps_per_execution=num_train_steps)"
      ],
      "metadata": {
        "id": "4iGxbmTePr12"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YGv8jQFQcG6",
        "outputId": "70aac563-4b04-47b7-c674-748cacd87b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ]
    }
  ]
}